{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKNEwbGikYyA",
        "outputId": "7bef1a90-6ce6-450c-ec81-3b05d236a7ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ Installing Apache Airflow (this takes ~1 min)...\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m70.7/70.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m591.3/591.3 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m351.2/351.2 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.9/46.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m152.5/152.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for python-nvd3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires sqlalchemy<3.0.0,>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mâš™ï¸ Initializing Database...\n",
            "ğŸ‘¤ Creating Admin User...\n",
            "âœ… Airflow Installed & Configured.\n",
            "ğŸ‘‰ Username: admin\n",
            "ğŸ‘‰ Password: admin\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Install Apache Airflow & Setup Environment\n",
        "import os\n",
        "\n",
        "# 1. Install Airflow and pyngrok (for the UI tunnel)\n",
        "print(\"â³ Installing Apache Airflow (this takes ~1 min)...\")\n",
        "!pip install -q apache-airflow==2.7.2 pyngrok pandas\n",
        "\n",
        "# 2. Set Airflow Home directory\n",
        "os.environ[\"AIRFLOW_HOME\"] = \"/content/airflow\"\n",
        "\n",
        "# 3. Initialize the Database (SQLite)\n",
        "print(\"âš™ï¸ Initializing Database...\")\n",
        "!airflow db init > /dev/null 2>&1\n",
        "\n",
        "# 4. Create an Admin User for the UI\n",
        "print(\"ğŸ‘¤ Creating Admin User...\")\n",
        "!airflow users create \\\n",
        "    --username admin \\\n",
        "    --firstname System \\\n",
        "    --lastname Admin \\\n",
        "    --role Admin \\\n",
        "    --email admin@example.com \\\n",
        "    --password admin \\\n",
        "    > /dev/null 2>&1\n",
        "\n",
        "# 5. Create directory for DAGs (Workflows)\n",
        "!mkdir -p /content/airflow/dags\n",
        "\n",
        "print(\"âœ… Airflow Installed & Configured.\")\n",
        "print(\"ğŸ‘‰ Username: admin\")\n",
        "print(\"ğŸ‘‰ Password: admin\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Define the Orchestration Logic (The DAG)\n",
        "# We write this python code into the Airflow DAGs folder\n",
        "\n",
        "dag_content = \"\"\"\n",
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "from airflow.operators.bash import BashOperator\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "\n",
        "# --- SIMULATED BUSINESS LOGIC ---\n",
        "\n",
        "def generate_order(**context):\n",
        "    order_id = f\"ORD-{random.randint(10000, 99999)}\"\n",
        "    amount = random.randint(50, 500)\n",
        "    print(f\"ğŸ›’ Order Generated: {order_id} for ${amount}\")\n",
        "\n",
        "    # Pass data to next task via XCom (Airflow's internal data sharing)\n",
        "    context['ti'].xcom_push(key='order_data', value={'id': order_id, 'amount': amount})\n",
        "\n",
        "def process_payment(**context):\n",
        "    # Get data from previous task\n",
        "    order_data = context['ti'].xcom_pull(key='order_data', task_ids='generate_order_task')\n",
        "    order_id = order_data['id']\n",
        "    print(f\"ğŸ’³ Processing payment for {order_id}...\")\n",
        "\n",
        "    time.sleep(1) # Simulate API call\n",
        "\n",
        "    # Simulate random failure to show how Orchestration handles errors\n",
        "    if random.random() < 0.2:\n",
        "        raise ValueError(\"âŒ Payment Gateway Timeout! (Simulated Failure)\")\n",
        "\n",
        "    print(\"âœ… Payment Authorized.\")\n",
        "    return \"PAID\"\n",
        "\n",
        "def check_inventory(**context):\n",
        "    order_data = context['ti'].xcom_pull(key='order_data', task_ids='generate_order_task')\n",
        "    print(f\"ğŸ“¦ Checking warehouse for {order_data['id']}...\")\n",
        "    time.sleep(1)\n",
        "    print(\"âœ… Stock Allocated.\")\n",
        "\n",
        "def ship_order(**context):\n",
        "    order_data = context['ti'].xcom_pull(key='order_data', task_ids='generate_order_task')\n",
        "    tracking_num = f\"TRK-{random.randint(100,999)}\"\n",
        "    print(f\"ğŸšš Order {order_data['id']} Shipped! Tracking: {tracking_num}\")\n",
        "\n",
        "# --- ORCHESTRATION DEFINITION (DAG) ---\n",
        "\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2023, 1, 1),\n",
        "    'retries': 2,                     # REAL ORCHESTRATION: Auto-retry on failure\n",
        "    'retry_delay': timedelta(seconds=5),\n",
        "}\n",
        "\n",
        "with DAG(\n",
        "    'ecommerce_fulfillment_pipeline',\n",
        "    default_args=default_args,\n",
        "    description='A simulated E-Commerce Saga',\n",
        "    schedule_interval=timedelta(seconds=30), # Run every 30 seconds automatically\n",
        "    catchup=False,\n",
        "    tags=['production', 'sales'],\n",
        ") as dag:\n",
        "\n",
        "    # 1. Define Tasks\n",
        "    t1 = PythonOperator(\n",
        "        task_id='generate_order_task',\n",
        "        python_callable=generate_order,\n",
        "    )\n",
        "\n",
        "    t2 = PythonOperator(\n",
        "        task_id='process_payment_task',\n",
        "        python_callable=process_payment,\n",
        "    )\n",
        "\n",
        "    t3 = PythonOperator(\n",
        "        task_id='check_inventory_task',\n",
        "        python_callable=check_inventory,\n",
        "    )\n",
        "\n",
        "    t4 = PythonOperator(\n",
        "        task_id='ship_order_task',\n",
        "        python_callable=ship_order,\n",
        "    )\n",
        "\n",
        "    t5 = BashOperator(\n",
        "        task_id='notify_customer',\n",
        "        bash_command='echo \"ğŸ“§ Email sent to customer!\"',\n",
        "    )\n",
        "\n",
        "    # 2. Define The Flow (The Orchestration)\n",
        "    # Order -> Payment -> Inventory -> Shipping -> Notification\n",
        "    t1 >> t2 >> t3 >> t4 >> t5\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Write the file to the Airflow DAGs directory\n",
        "with open(\"/content/airflow/dags/ecommerce_pipeline.py\", \"w\") as f:\n",
        "    f.write(dag_content)\n",
        "\n",
        "print(\"âœ… DAG file created at /content/airflow/dags/ecommerce_pipeline.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxLq8iaSktqG",
        "outputId": "cc671153-896d-4d19-ea76-b73fa8347b9f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… DAG file created at /content/airflow/dags/ecommerce_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Start Airflow Webserver & Scheduler\n",
        "import subprocess\n",
        "import time\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "# --- USER INPUT REQUIRED ---\n",
        "NGROK_AUTH_TOKEN = \"2lByyY668AQEl2B574HOizPVWze_   3roGxEj1UQdg2rXyx7tH8\" # @param {type:\"string\"}\n",
        "\n",
        "if NGROK_AUTH_TOKEN == \"PASTE_YOUR_NGROK_TOKEN_HERE\" or NGROK_AUTH_TOKEN == \"\":\n",
        "    print(\"âŒ Error: You must paste your Ngrok Authtoken above to see the UI.\")\n",
        "    print(\"get it for free at https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "else:\n",
        "    # 1. Authenticate ngrok\n",
        "    conf.get_default().auth_token = NGROK_AUTH_TOKEN\n",
        "\n",
        "    # 2. Kill any old processes\n",
        "    !pkill -f airflow\n",
        "\n",
        "    # 3. Start Scheduler (The Heart)\n",
        "    print(\"ğŸ’“ Starting Airflow Scheduler (Background)...\")\n",
        "    subprocess.Popen([\"airflow\", \"scheduler\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "    # 4. Start Webserver (The UI)\n",
        "    print(\"ğŸ–¥ï¸ Starting Airflow Webserver (Background)...\")\n",
        "    subprocess.Popen([\"airflow\", \"webserver\", \"--port\", \"8080\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "    # 5. Create Tunnel\n",
        "    print(\"ğŸ”— Creating secure tunnel...\")\n",
        "    time.sleep(10) # Wait for webserver to boot\n",
        "    try:\n",
        "        public_url = ngrok.connect(8080).public_url\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"ğŸš€ AIRFLOW UI IS LIVE: {public_url}\")\n",
        "        print(\"=\"*50)\n",
        "        print(\"1. Click the link above.\")\n",
        "        print(\"2. Login with -> User: admin | Pass: admin\")\n",
        "        print(\"3. Toggle the 'ecommerce_fulfillment_pipeline' switch to ON (Left side).\")\n",
        "        print(\"4. Click the DAG name to see the Graph/Grid view.\")\n",
        "        print(\"=\"*50)\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Error opening tunnel: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BzBMhXAkyi2",
        "outputId": "e96c6d61-21e3-4985-e603-fc5d4e598690"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’“ Starting Airflow Scheduler (Background)...\n",
            "ğŸ–¥ï¸ Starting Airflow Webserver (Background)...\n",
            "ğŸ”— Creating secure tunnel...\n",
            "\n",
            "==================================================\n",
            "ğŸš€ AIRFLOW UI IS LIVE: https://11892bd1ee23.ngrok-free.app\n",
            "==================================================\n",
            "1. Click the link above.\n",
            "2. Login with -> User: admin | Pass: admin\n",
            "3. Toggle the 'ecommerce_fulfillment_pipeline' switch to ON (Left side).\n",
            "4. Click the DAG name to see the Graph/Grid view.\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Fix Dependencies & Install Airflow\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# 1. Uninstall conflicting libraries provided by Colab defaults\n",
        "print(\"ğŸ§¹ Cleaning up conflicting libraries...\")\n",
        "!pip uninstall -y sqlalchemy opentelemetry-api opentelemetry-sdk opentelemetry-exporter-gcp-logging google-adk ipython-sql > /dev/null 2>&1\n",
        "\n",
        "# 2. Install Airflow using the OFFICIAL constraints file\n",
        "# This forces the versions to match exactly what Airflow needs\n",
        "print(\"â³ Installing Apache Airflow (Robust Mode)...\")\n",
        "AIRFLOW_VERSION = '2.7.2'\n",
        "PYTHON_VERSION = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
        "CONSTRAINT_URL = f\"https://raw.githubusercontent.com/apache/airflow/constraints-{AIRFLOW_VERSION}/constraints-{PYTHON_VERSION}.txt\"\n",
        "\n",
        "!pip install \"apache-airflow=={AIRFLOW_VERSION}\" --constraint \"{CONSTRAINT_URL}\" pandas pyngrok > /dev/null 2>&1\n",
        "\n",
        "# 3. Initialize Environment\n",
        "os.environ[\"AIRFLOW_HOME\"] = \"/content/airflow\"\n",
        "# Fix specific SQLAlchemy error in Colab\n",
        "os.environ[\"AIRFLOW__CORE__SQL_ALCHEMY_CONN\"] = \"sqlite:////content/airflow/airflow.db\"\n",
        "os.environ[\"AIRFLOW__CORE__LOAD_EXAMPLES\"] = \"False\" # Don't load example DAGs (saves memory)\n",
        "\n",
        "print(\"âœ… Installation Successful.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SArGj6ysloxK",
        "outputId": "da62d270-d144-4391-e141-e0af2af821fa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§¹ Cleaning up conflicting libraries...\n",
            "â³ Installing Apache Airflow (Robust Mode)...\n",
            "âœ… Installation Successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Initialize Database & Create Admin\n",
        "import os\n",
        "\n",
        "# 1. Initialize DB\n",
        "print(\"âš™ï¸ Initializing SQLite Database...\")\n",
        "!airflow db init > /dev/null 2>&1\n",
        "\n",
        "# 2. Create Admin User\n",
        "print(\"ğŸ‘¤ Creating Admin User...\")\n",
        "!airflow users create \\\n",
        "    --username admin \\\n",
        "    --firstname System \\\n",
        "    --lastname Admin \\\n",
        "    --role Admin \\\n",
        "    --email admin@example.com \\\n",
        "    --password admin \\\n",
        "    > /dev/null 2>&1\n",
        "\n",
        "# 3. Create DAGs folder\n",
        "!mkdir -p /content/airflow/dags\n",
        "\n",
        "print(\"âœ… Airflow Configured.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MOAASfPlrHk",
        "outputId": "6456aa2f-d6fd-44e3-a541-66cc72514656"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš™ï¸ Initializing SQLite Database...\n",
            "ğŸ‘¤ Creating Admin User...\n",
            "âœ… Airflow Configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Create the Orchestration DAG\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "dag_content = \"\"\"\n",
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "from airflow.operators.bash import BashOperator\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "\n",
        "def generate_order(**context):\n",
        "    order_id = f\"ORD-{random.randint(10000, 99999)}\"\n",
        "    amount = random.randint(50, 500)\n",
        "    print(f\"ğŸ›’ Order Generated: {order_id} for ${amount}\")\n",
        "    context['ti'].xcom_push(key='order_data', value={'id': order_id, 'amount': amount})\n",
        "\n",
        "def process_payment(**context):\n",
        "    order_data = context['ti'].xcom_pull(key='order_data', task_ids='generate_order_task')\n",
        "    print(f\"ğŸ’³ Processing payment for {order_data['id']}...\")\n",
        "    time.sleep(1)\n",
        "\n",
        "    # 20% Chance of failure to show Orchestration Retry Logic\n",
        "    if random.random() < 0.2:\n",
        "        raise ValueError(\"âŒ Payment Gateway Timeout!\")\n",
        "\n",
        "    print(\"âœ… Payment Authorized.\")\n",
        "\n",
        "def check_inventory(**context):\n",
        "    order_data = context['ti'].xcom_pull(key='order_data', task_ids='generate_order_task')\n",
        "    print(f\"ğŸ“¦ Checking warehouse for {order_data['id']}...\")\n",
        "    time.sleep(1)\n",
        "    print(\"âœ… Stock Allocated.\")\n",
        "\n",
        "def ship_order(**context):\n",
        "    order_data = context['ti'].xcom_pull(key='order_data', task_ids='generate_order_task')\n",
        "    tracking_num = f\"TRK-{random.randint(100,999)}\"\n",
        "    print(f\"ğŸšš Order {order_data['id']} Shipped! Tracking: {tracking_num}\")\n",
        "\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2023, 1, 1),\n",
        "    'retries': 2,\n",
        "    'retry_delay': timedelta(seconds=2),\n",
        "}\n",
        "\n",
        "with DAG(\n",
        "    'ecommerce_fulfillment_pipeline',\n",
        "    default_args=default_args,\n",
        "    description='A simulated E-Commerce Saga',\n",
        "    schedule_interval=timedelta(seconds=30),\n",
        "    catchup=False,\n",
        "    tags=['production'],\n",
        ") as dag:\n",
        "\n",
        "    t1 = PythonOperator(task_id='generate_order_task', python_callable=generate_order)\n",
        "    t2 = PythonOperator(task_id='process_payment_task', python_callable=process_payment)\n",
        "    t3 = PythonOperator(task_id='check_inventory_task', python_callable=check_inventory)\n",
        "    t4 = PythonOperator(task_id='ship_order_task', python_callable=ship_order)\n",
        "\n",
        "    t1 >> t2 >> t3 >> t4\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/content/airflow/dags/ecommerce_pipeline.py\", \"w\") as f:\n",
        "    f.write(dag_content)\n",
        "\n",
        "print(\"âœ… DAG file created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQkmHVc5ltq_",
        "outputId": "75f320e7-4b6c-4686-fc8f-3227a55227ad"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… DAG file created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Start Server & Open Tunnel (With Health Check)\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "# --- PASTE YOUR TOKEN BELOW ---\n",
        "NGROK_AUTH_TOKEN = \"2lByyY668AQEl2B574HOizPVWze_     3roGxEj1UQdg2rXyx7tH8\"\n",
        "# ------------------------------\n",
        "\n",
        "if NGROK_AUTH_TOKEN == \"PASTE_YOUR_NGROK_TOKEN_HERE\" or NGROK_AUTH_TOKEN == \"\":\n",
        "    print(\"âŒ You missed the Ngrok Token. Please paste it in the code above.\")\n",
        "else:\n",
        "    # 1. Kill old processes\n",
        "    !pkill -f airflow\n",
        "\n",
        "    # 2. Start Scheduler\n",
        "    print(\"ğŸ’“ Starting Scheduler...\")\n",
        "    subprocess.Popen([\"airflow\", \"scheduler\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "    # 3. Start Webserver\n",
        "    print(\"ğŸ–¥ï¸ Starting Webserver (this takes ~30 seconds)...\")\n",
        "    subprocess.Popen([\"airflow\", \"webserver\", \"--port\", \"8080\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "    # 4. HEALTH CHECK LOOP\n",
        "    # We wait until localhost:8080 responds before creating the tunnel\n",
        "    print(\"â³ Waiting for Airflow to boot up...\")\n",
        "    server_ready = False\n",
        "    for i in range(20): # Try for 100 seconds (20 * 5)\n",
        "        try:\n",
        "            r = requests.get(\"http://localhost:8080/health\")\n",
        "            if r.status_code == 200:\n",
        "                server_ready = True\n",
        "                break\n",
        "        except:\n",
        "            pass\n",
        "        time.sleep(5)\n",
        "        print(\".\", end=\"\")\n",
        "\n",
        "    if server_ready:\n",
        "        conf.get_default().auth_token = NGROK_AUTH_TOKEN\n",
        "        public_url = ngrok.connect(8080).public_url\n",
        "        print(\"\\n\\n\" + \"=\"*50)\n",
        "        print(f\"ğŸš€ AIRFLOW IS LIVE: {public_url}\")\n",
        "        print(\"=\"*50)\n",
        "        print(\"Login: admin / admin\")\n",
        "        print(\"1. Toggle the switch ON for 'ecommerce_fulfillment_pipeline'\")\n",
        "        print(\"2. Click the DAG name -> Click 'Graph' tab\")\n",
        "        print(\"3. Click the 'Play' button (top right) to trigger manually\")\n",
        "    else:\n",
        "        print(\"\\nâŒ Timed out waiting for Airflow to start. Check logs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxKYd6fllw3k",
        "outputId": "02f8e130-39f9-4f6e-8977-d627d377c9e9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’“ Starting Scheduler...\n",
            "ğŸ–¥ï¸ Starting Webserver (this takes ~30 seconds)...\n",
            "â³ Waiting for Airflow to boot up...\n",
            "....................\n",
            "âŒ Timed out waiting for Airflow to start. Check logs.\n"
          ]
        }
      ]
    }
  ]
}